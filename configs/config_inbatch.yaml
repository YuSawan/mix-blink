# Model Configuration
model:
  mention_encoder: 'google-bert/bert-large-uncased' # Hugging Face model
  entity_encoder: 'google-bert/bert-large-uncased' # Hugging Face model
  hidden_size: 1024 # 768 if you use base
  freeze_mention_encoder: false
  freeze_entity_encoder: false
  mention_context_length: 512
  entity_context_length: 512
  measure: 'cos' # measure: {'cos': cosine, 'ip': inner-product, 'l2': eucridean}
  temperature: 1.0 # temperature: temperature for ranging similarity
  negative: False # negative: {False: in-batch sampling, True: inbatch+hard negatives}
  top_k: 10 # top_k: Number of hard negative samples
  prev_path: null # Pretrained Model Path: Use None if no pretrained model is being used
  cache_dir: null

# Dataset Configuration
dataset:
  train_file: './zelda/zelda_train.jsonl'
  test_file: './zelda/test_aida-b.jsonl'
  dictionary_file: './zelda/zelda_dictionary.jsonl'
  add_nil: true
  nil_label: '[NIL]'
  nil_description: '[NIL] is an entity that does not exist in the dataset.'
  start_mention_token: '<mention>'
  end_mention_token: '</mention>'
  entity_token: '<entity>'

# Dataloader
remove_unused_columns: false

# Training Parameters
num_train_epochs: 5
per_device_train_batch_size: 8
per_device_eval_batch_size: 32
lr_scheduler_type: "linear"
warmup_ratio: 0.06

# Optimizer
optim: "adamw_torch"
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1.e-6

# Learning Rate and weight decay Configuration
learning_rate: 1.e-5
weight_decay: 0.01
max_grad_norm: 0.0

# logging
log_level: 'info'
logging_strategy: 'epoch'
logging_steps: 10
report_to: 'wandb'

# Save
save_strategy: 'epoch'
save_total_limit: 3 #maximum amount of checkpoints to save

# Evaluation
eval_strategy: 'epoch'
metric_for_best_model: 'recall'
load_best_model_at_end: true
eval_on_start: false
